% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\begin{fullwidth}

\section*{\smallcaps{Acknowledgement}}
The contents of this summary are based on the ``Algebra'' lecture given by \mbox{Frank Himstedt} at the Technical University of Munich in fall of 2020.

\vfill

\thispagestyle{empty}
\setlength{\parindent}{0pt}

\par\noindent\@title
\selectfont\par\noindent\@author
\selectfont\par\noindent\@date

% \subsection*{\smallcaps{Correspondence to lectures}}
% Introduction \quad \cref{sec:fundamentals}\\
% Bayesian Linear Regression \quad \cref{sec:blr} to \cref{sec:blr:online} and \cref{sec:kf}\\
% Gaussian Processes 1 \quad \cref{sec:blr:kernel_trick} to \cref{sec:blr:function_space_view} and \cref{sec:gp} to \cref{sec:gp:sampling}\\
% Gaussian Processes 2 \quad \cref{sec:gp:model_selection} to \cref{sec:gp:approximations}\\
% Variational Inference \quad \cref{sec:approximate_inference} to \cref{sec:approximate_inference:variational_inference:elbo}\\
% Markov Chain Monte Carlo \quad \cref{sec:approximate_inference:variational_inference:gradient_of_elbo} to \cref{sec:approximate_inference:mcmc}\\
% Bayesian Deep Learning \quad \cref{sec:bdl}\\
% Active Learning \quad \cref{sec:active_learning} to \cref{sec:bayesian_optimization}\\
% Markov Decision Processes \quad \cref{sec:mdp}\\
% Reinforcement Learning 1 \quad \cref{sec:tabular_rl} to \cref{sec:tabular_rl:model_free:q_learning}\\
% Reinforcement Learning 2 \quad \cref{sec:tabular_rl:model_free:off_policy_value_estimation} to \cref{sec:mfarl:policy_approximation}\\
% Reinforcement Learning 3 \quad \cref{sec:mfarl:actor_critic_methods}\\
% Reinforcement Learning 4 \quad \cref{sec:mbarl}

% \subsection*{\smallcaps{Jupyter Notebooks with examples}}
% \url{https://gitlab.inf.ethz.ch/OU-KRAUSE/pai-demos}

\end{fullwidth}
